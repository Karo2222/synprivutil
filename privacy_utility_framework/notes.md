Some of my notes, ignore:

**Data Cleaning**
Handle Missing Values:

Identify and handle missing data by either imputing missing values, dropping missing records, (or: using models that can handle missing data.)
(For example: df.fillna(value, inplace=True) or df.dropna(inplace=True).)

Remove Duplicates:

no unintended duplicate rows (can distort the synthetic data generation.)
Example: df.drop_duplicates(inplace=True).

Correct Data Types:

each column has the appropriate data type (e.g., numeric, categorical, datetime) (for correct modeling and generating synthetic data)
for example: df['age'] = df['age'].astype(int). (guess the column type based on the data and look for entries that don't match the data type of the other values)

Handle Outliers:

identify/handle outliers which could skew the synthetic data generation process (depending on the context, remove or transform outliers?)
for example: df = df[df['column'] < threshold].


- Rewrite functions into classes? Make the code more general (?)
- New way of sd generation, combine different test parts into one example, especially clean up all of the csv files
- transformation (scaling, normalizing)


Gaussian Mixture

Encoder file not found. Proceeding without encoder.
Scaler file not found. Proceeding without scaler.
Average DCR (Euclidean): 0.1986112337997193
Average DCR (Manhattan): 0.43699876617501854
Average DCR (Hamming): 1.0


CTGAN

Encoder file not found. Proceeding without encoder.
Scaler loaded: MinMaxScaler
Average DCR (Euclidean): 0.27297529061687636
Average DCR (Manhattan): 0.5897055078252942
Average DCR (Hamming): 0.7887731481481483




synthetic_data.csv & diabetes.csv:

Average DCR (Euclidean): 15.79299650086864
Average DCR (Manhattan): 31.748280891953808

Average DCR (Euclidean): 0.1914807063427121
Average DCR (Manhattan): 0.41247199150260777

NNDR: 0.8310473971250847
The Wasserstein distance is: 27.862384542060358

synthetic_data_ctgan.csv & diabetes.csv:

Average DCR (Euclidean): 22.079282218312482
Average DCR (Manhattan): 45.025831312731945

Average DCR (Euclidean): 0.27297529061687636
Average DCR (Manhattan): 0.5897055078252942

NNDR: 0.879017393602902
The Wasserstein distance is: 48.988064820467



synthetic/ctgan/normalized & diabetes/normalized:

Nearest Neighbor Adversarial Accuracy (NNAA): 0.6790364583333333

synthetic/gaussian mixture/normalized & diabetes/normalized:

Nearest Neighbor Adversarial Accuracy (NNAA): 0.525390625




wasserstein-distance: change to multidimensional

- some new (privacy) metrics, adapt privacy metrics (weighted, privacy protection in different labels)
- limitations of existing metrics? (threshold, propose method of finding out what a threshold could be?), in niche datasets (let the user specify which columns are more important, whcih ones less important)
- different models, compare
- show evaluation of different synthetic data sets (generated by models, or downloaded synthetic datasets)


Resemblance. The resemblance metric measures how closely the distribution and inter-correlation of the columns in the synthetic data match the original data, ensuring that the synthetic data cap- tures the statistical patterns and characteristics of the original data. Our resemblance metric is composed of five similarity measures:
• Column Similarity calculates the correlation between each orig- inal and synthetic column, using Pearson’s coefficient for numeri- cal columns and Theil’s U for categorical columns.
• Correlation Similarity measures the correlation between the correlation coefficients of each column pair. First, the Pearson correlation for numerical pairs, Theil’s U for categorical pairs, and the correlation ratio for mixed cases are calculated. Then, the correlation between these coefficients is calculated.
• Statistical Similarity employs Spearman’s Rho to correlate de- scriptive statistics (minimum, maximum, median, mean, and stan- dard deviation) of numerical columns in synthetic and original data.
1https://github.com/sdv-dev/SDV
2 https://github.com/vanderschaarlab/synthcity
• Jensen-Shannon Similarity uses the Jensen-Shannon distance between the probability distributions of the original and synthetic columns. One minus this distance is used so that higher scores are better, as in the other metrics.
• Kolmogorov-SmirnovSimilarityusestheKolmogorov-Smirnov distance to measure the maximum difference between the cumu- lative distributions of each original and synthetic column. Once again, one minus the distance is used so that a higher score is better.





Mindmap, utility-privacy-metrics
check wasserstein